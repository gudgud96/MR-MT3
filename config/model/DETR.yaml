_target_: tasks.detr.DETR
config: # a hacky way to make this config works for T5 class
  architectures:
    - T5DETR
  d_ff: 1024
  d_kv: 64
  d_model: 512
  decoder_start_token_id: 0
  dropout_rate: 0.1
  pad_token_id: 0
  eos_token_id: 1
  unk_token_id: 2
  feed_forward_proj: gated-gelu
  initializer_factor: 1.0
  is_encoder_decoder: true
  layer_norm_epsilon: 1e-06
  model_type: t5
  num_heads: 6
  num_decoder_layers: 8
  num_layers: 8
  output_past: true
  tie_word_embeddings: false
  vocab_size_pitch: 128 # 128 pitch + 1 not found token
  vocab_size_program: 128 # 128 instruments + 1 drum + 1 not found token
  vocab_size_onset: ${mel_length} # number of steps in one segment
  vocab_size_offset: ${mel_length} # number of steps in one segment
  encoder_vocab_size: 1024
  xl_context_length: 2048
  # new parameters for DETR
  detr_num_tokens: ${event_length}
  detr_hidden_dim: ${model.config.d_model}
optim_cfg: ${optim}